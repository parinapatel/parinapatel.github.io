{"componentChunkName":"component---src-pages-pensieve-index-js","path":"/pensieve/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"title":"MultiTenancy on Kubernetes","description":"Soft Multitenancy on Kubernetes","slug":"/pensieve/docker-error","date":"2020-02-03","tags":["Kubernetes","Docker"],"draft":false},"html":"<h2>Problem</h2>\n<p>Workload for Client specific task can be made to run on client specific hardware , They can be achieved by Node Selector or Taint / Tolerances , Both path way have perk and cons , This document provides that information.</p>\n<p>Sometimes client have their own hardware where their task , Apps and other long running workload run. Current Rancher master has reached EOL and we are planning to move to current K8s Platform . Due to better functionality and redundancy resource requirements have increased thus we are looking for better solution for client cluster. Finally solution is to merge those node into MT K8s cluster but still keeping them segregated by VLANs and some mechanism for separation of workload .</p>\n<p>Following methods can be used for workload segregation.</p>\n<p>Node Selectors via Labels / Affinity : <a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity</a></p>\n<p>Taint and Tolerances : <a href=\"https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</a></p>\n<h2>Solution</h2>\n<h3>What is multi-tenant Kubernetes?</h3>\n<ul>\n<li>Multi-tenant Kubernetes is a Kubernetes deployment where multiple applications or workloads run side-by-side.</li>\n<li>Multi-tenancy is a common architecture for organizations that have multiple applications running in the same environment, or where different teams (like developers and IT Ops) share the same Kubernetes environment.</li>\n<li>You can think of multi-tenant Kubernetes as being akin to an apartment building, whereas single-tenant Kubernetes is like a single-family house.</li>\n</ul>\n<h3>Option 1 : Taint &#x26; Tolerances</h3>\n<h4>Description:</h4>\n<p>Taints are a Kubernetes feature that repel pods from nodes. When a node receives a taint, all pods that do not have a matching tolerance will be repelled from that node.</p>\n<p>This method works as <b>DENY ALL UNLESS ALLOWED</b> .</p>\n<p>Example :</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># Node Taint</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> kubectl taint nodes test-node-1 my-taint<span class=\"token operator\">=</span>test:NoSchedule\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> kubectl describe node ip-192-168-101-21.us-west-2.compute.internal\n\nName:               test-node-1\nRoles:              <span class=\"token operator\">&lt;</span>none<span class=\"token operator\">></span>\nLabels:             beta.kubernetes.io/arch<span class=\"token operator\">=</span>amd64\n                    beta.kubernetes.io/instance-type<span class=\"token operator\">=</span>m4.xlarge\n                    beta.kubernetes.io/os<span class=\"token operator\">=</span>linux\n                    failure-domain.beta.kubernetes.io/region<span class=\"token operator\">=</span>us-west-2\n                    failure-domain.beta.kubernetes.io/zone<span class=\"token operator\">=</span>us-west-2a\n                    kubernetes.io/hostname<span class=\"token operator\">=</span>ip-192-168-101-21.us-west-2.compute.internal\n                    pipeline-nodepool-name<span class=\"token operator\">=</span>pool1\nAnnotations:        node.alpha.kubernetes.io/ttl<span class=\"token operator\">=</span><span class=\"token number\">0</span>\n                    volumes.kubernetes.io/controller-managed-attach-detach<span class=\"token operator\">=</span>true\nCreationTimestamp:  Wed, <span class=\"token number\">29</span> Aug <span class=\"token number\">2018</span> <span class=\"token number\">11</span>:31:53 +0200\nTaints:             my-taint<span class=\"token operator\">=</span>test:NoSchedule\n\n\n<span class=\"token comment\">#Pod Tolerance</span>\n    spec:\n     tolerations:\n      - key: <span class=\"token string\">\"my-taint\"</span>\n        operator: Equal\n        value: <span class=\"token string\">\"test\"</span></code></pre></div>\n<h4>Pro:</h4>\n<p>Pods from other orgs/namespaces are guaranteed to not schedule on nodes that are owned by certain clients. Best security and workload isolation between clients.</p>\n<ul>\n<li>This will require the least amount of work on software side. Only pods by clients would need changes to their pod configuration. ( Its same amount either way ;p )</li>\n</ul>\n<h4>Cons:</h4>\n<ul>\n<li>In case of failure of nodes , Spilling Client work on MT cluster will be extremely difficult</li>\n<li>Debugging failure of container by starting test container will require tolerance thus additional configuration by pod ( lots of work)</li>\n<li>Starting Client specific workload by helm requires additional configuration</li>\n<li>Each of Master/ System level config needs to be added tolerance for given taints.</li>\n</ul>\n<h3>Option 2 : Node Selectors via Labels / Affinity</h3>\n<h4>Description:</h4>\n<p>Here each pod will start on only nodes labeled with certain labels.</p>\n<p>Example :</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">affinity</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">nodeAffinity</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">requiredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">nodeSelectorTerms</span><span class=\"token punctuation\">:</span>\n        <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n          <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> kubernetes.io/e2e<span class=\"token punctuation\">-</span>az<span class=\"token punctuation\">-</span>name\n            <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n            <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> e2e<span class=\"token punctuation\">-</span>az1\n            <span class=\"token punctuation\">-</span> e2e<span class=\"token punctuation\">-</span>az2\n      <span class=\"token key atrule\">preferredDuringSchedulingIgnoredDuringExecution</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">weight</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span>\n        <span class=\"token key atrule\">preference</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">matchExpressions</span><span class=\"token punctuation\">:</span>\n          <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> another<span class=\"token punctuation\">-</span>node<span class=\"token punctuation\">-</span>label<span class=\"token punctuation\">-</span>key\n            <span class=\"token key atrule\">operator</span><span class=\"token punctuation\">:</span> In\n            <span class=\"token key atrule\">values</span><span class=\"token punctuation\">:</span>\n            <span class=\"token punctuation\">-</span> another<span class=\"token punctuation\">-</span>node<span class=\"token punctuation\">-</span>label<span class=\"token punctuation\">-</span>value\n  <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> with<span class=\"token punctuation\">-</span>node<span class=\"token punctuation\">-</span>affinity\n    <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> k8s.gcr.io/pause<span class=\"token punctuation\">:</span><span class=\"token number\">2.0</span>\n\nSimpler Config<span class=\"token punctuation\">:</span>\n\n<span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n  <span class=\"token key atrule\">labels</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">env</span><span class=\"token punctuation\">:</span> test\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx\n    <span class=\"token key atrule\">imagePullPolicy</span><span class=\"token punctuation\">:</span> IfNotPresent\n  <span class=\"token key atrule\">nodeSelector</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">disktype</span><span class=\"token punctuation\">:</span> ssd</code></pre></div>\n<h4>Pro:</h4>\n<ul>\n<li>Better fault tolerance and greater capacity for all clients. If a tainted node fails, other nodes could have trouble starting up all failed pods which would increase recovery time.</li>\n<li>Moving Workload around during failure is lot easier.</li>\n<li>Better ease in starting debug workloads.</li>\n<li>Easier and Detailed configuration for rules including weight based scheduling</li>\n<li>Future support for APPs Frame work.</li>\n</ul>\n<h4>Cons:</h4>\n<ul>\n<li>Node selector have chance to start on different client node if workload has not specified to start on MT nodes. ( MOSTLY MANUALLY STARTED PODS for debugging : This should be reduces as much as we can . )</li>\n<li>Node selector can run into extreme detailed configuration ( hard to grasp)</li>\n</ul>"}}]}},"pageContext":{}}}